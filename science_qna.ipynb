{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34e2d2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rckyi/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForQuestionAnswering, AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "# Load ScienceQA dataset (without streaming)\n",
    "dataset = load_dataset(\"derek-thomas/ScienceQA\", split={\"train\": \"train\", \"validation\": \"validation\"})\n",
    "\n",
    "# Use DistilBERT for Question Answering\n",
    "MODEL_NAME = \"distilbert-base-cased-distilled-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1142893",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c26eafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "  param.requires_grad = False  # freeze the model - train adapters later\n",
    "  if param.ndim == 1:\n",
    "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
    "    param.data = param.data.to(torch.float32)\n",
    "\n",
    "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "  def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "\n",
    "# model.lm_head = CastOutputToFloat(model.lm_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78af539d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "distilbert\n",
      "distilbert.embeddings\n",
      "distilbert.embeddings.word_embeddings\n",
      "distilbert.embeddings.position_embeddings\n",
      "distilbert.embeddings.LayerNorm\n",
      "distilbert.embeddings.dropout\n",
      "distilbert.transformer\n",
      "distilbert.transformer.layer\n",
      "distilbert.transformer.layer.0\n",
      "distilbert.transformer.layer.0.attention\n",
      "distilbert.transformer.layer.0.attention.dropout\n",
      "distilbert.transformer.layer.0.attention.q_lin\n",
      "distilbert.transformer.layer.0.attention.k_lin\n",
      "distilbert.transformer.layer.0.attention.v_lin\n",
      "distilbert.transformer.layer.0.attention.out_lin\n",
      "distilbert.transformer.layer.0.sa_layer_norm\n",
      "distilbert.transformer.layer.0.ffn\n",
      "distilbert.transformer.layer.0.ffn.dropout\n",
      "distilbert.transformer.layer.0.ffn.lin1\n",
      "distilbert.transformer.layer.0.ffn.lin2\n",
      "distilbert.transformer.layer.0.ffn.activation\n",
      "distilbert.transformer.layer.0.output_layer_norm\n",
      "distilbert.transformer.layer.1\n",
      "distilbert.transformer.layer.1.attention\n",
      "distilbert.transformer.layer.1.attention.dropout\n",
      "distilbert.transformer.layer.1.attention.q_lin\n",
      "distilbert.transformer.layer.1.attention.k_lin\n",
      "distilbert.transformer.layer.1.attention.v_lin\n",
      "distilbert.transformer.layer.1.attention.out_lin\n",
      "distilbert.transformer.layer.1.sa_layer_norm\n",
      "distilbert.transformer.layer.1.ffn\n",
      "distilbert.transformer.layer.1.ffn.dropout\n",
      "distilbert.transformer.layer.1.ffn.lin1\n",
      "distilbert.transformer.layer.1.ffn.lin2\n",
      "distilbert.transformer.layer.1.ffn.activation\n",
      "distilbert.transformer.layer.1.output_layer_norm\n",
      "distilbert.transformer.layer.2\n",
      "distilbert.transformer.layer.2.attention\n",
      "distilbert.transformer.layer.2.attention.dropout\n",
      "distilbert.transformer.layer.2.attention.q_lin\n",
      "distilbert.transformer.layer.2.attention.k_lin\n",
      "distilbert.transformer.layer.2.attention.v_lin\n",
      "distilbert.transformer.layer.2.attention.out_lin\n",
      "distilbert.transformer.layer.2.sa_layer_norm\n",
      "distilbert.transformer.layer.2.ffn\n",
      "distilbert.transformer.layer.2.ffn.dropout\n",
      "distilbert.transformer.layer.2.ffn.lin1\n",
      "distilbert.transformer.layer.2.ffn.lin2\n",
      "distilbert.transformer.layer.2.ffn.activation\n",
      "distilbert.transformer.layer.2.output_layer_norm\n",
      "distilbert.transformer.layer.3\n",
      "distilbert.transformer.layer.3.attention\n",
      "distilbert.transformer.layer.3.attention.dropout\n",
      "distilbert.transformer.layer.3.attention.q_lin\n",
      "distilbert.transformer.layer.3.attention.k_lin\n",
      "distilbert.transformer.layer.3.attention.v_lin\n",
      "distilbert.transformer.layer.3.attention.out_lin\n",
      "distilbert.transformer.layer.3.sa_layer_norm\n",
      "distilbert.transformer.layer.3.ffn\n",
      "distilbert.transformer.layer.3.ffn.dropout\n",
      "distilbert.transformer.layer.3.ffn.lin1\n",
      "distilbert.transformer.layer.3.ffn.lin2\n",
      "distilbert.transformer.layer.3.ffn.activation\n",
      "distilbert.transformer.layer.3.output_layer_norm\n",
      "distilbert.transformer.layer.4\n",
      "distilbert.transformer.layer.4.attention\n",
      "distilbert.transformer.layer.4.attention.dropout\n",
      "distilbert.transformer.layer.4.attention.q_lin\n",
      "distilbert.transformer.layer.4.attention.k_lin\n",
      "distilbert.transformer.layer.4.attention.v_lin\n",
      "distilbert.transformer.layer.4.attention.out_lin\n",
      "distilbert.transformer.layer.4.sa_layer_norm\n",
      "distilbert.transformer.layer.4.ffn\n",
      "distilbert.transformer.layer.4.ffn.dropout\n",
      "distilbert.transformer.layer.4.ffn.lin1\n",
      "distilbert.transformer.layer.4.ffn.lin2\n",
      "distilbert.transformer.layer.4.ffn.activation\n",
      "distilbert.transformer.layer.4.output_layer_norm\n",
      "distilbert.transformer.layer.5\n",
      "distilbert.transformer.layer.5.attention\n",
      "distilbert.transformer.layer.5.attention.dropout\n",
      "distilbert.transformer.layer.5.attention.q_lin\n",
      "distilbert.transformer.layer.5.attention.k_lin\n",
      "distilbert.transformer.layer.5.attention.v_lin\n",
      "distilbert.transformer.layer.5.attention.out_lin\n",
      "distilbert.transformer.layer.5.sa_layer_norm\n",
      "distilbert.transformer.layer.5.ffn\n",
      "distilbert.transformer.layer.5.ffn.dropout\n",
      "distilbert.transformer.layer.5.ffn.lin1\n",
      "distilbert.transformer.layer.5.ffn.lin2\n",
      "distilbert.transformer.layer.5.ffn.activation\n",
      "distilbert.transformer.layer.5.output_layer_norm\n",
      "qa_outputs\n",
      "dropout\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92f8e04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c849960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 75266 || all params: 65267716 || trainable%: 0.11531888138999685\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType \n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16, #attention heads\n",
    "    lora_alpha=32, #alpha scaling\n",
    "    target_modules=[\"distilbert.transformer.layer.5.attention.q_lin\", \"distilbert.transformer.layer.5.attention.v_lin\", \"distilbert.transformer.layer.5.attention.out_lin\"], #if you know the \n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.QUESTION_ANS # set this for CLM or Seq2Seq\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "823f244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_answer_positions(context, answer_text):\n",
    "    \"\"\"Finds the start and end positions of the answer text within the tokenized context.\"\"\"\n",
    "    tokenized_context = tokenizer(context, add_special_tokens=False, return_offsets_mapping=True)\n",
    "    \n",
    "    # Convert answer to token IDs\n",
    "    answer_tokens = tokenizer(answer_text,  add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "    # Find start position\n",
    "    for i in range(len(tokenized_context[\"input_ids\"])):\n",
    "        if tokenized_context[\"input_ids\"][i:i+len(answer_tokens)] == answer_tokens:\n",
    "            return i, i + len(answer_tokens) - 1  # Return start & end positions\n",
    "\n",
    "    return None, None  # If answer not found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a6f60d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_function(examples):\n",
    "#     \"\"\"\n",
    "#     Extracts (question, choices), finds the correct answer, tokenizes, \n",
    "#     and returns training data in the format expected by DistilBERT.\n",
    "#     \"\"\"\n",
    "#     questions = examples[\"question\"]\n",
    "#     choices = examples[\"choices\"]\n",
    "#     answer_indices = examples[\"answer\"]\n",
    "\n",
    "#     tokenized_inputs = {\n",
    "#         \"input_ids\": [],\n",
    "#         \"attention_mask\": [],\n",
    "#         \"start_positions\": [],\n",
    "#         \"end_positions\": [],\n",
    "#     }\n",
    "\n",
    "#     for q, c, idx in zip(questions, choices, answer_indices):\n",
    "#         if idx < 0 or idx >= len(c):  # Handle bad indices\n",
    "#             continue\n",
    "\n",
    "#         correct_answer = c[idx]\n",
    "#         context = f\"{q} Choices: {', '.join(c)}\"\n",
    "#         print(f\"context {context}\")\n",
    "\n",
    "#         # Tokenize the entire question + choices\n",
    "#         tokenized = tokenizer(context, padding=\"max_length\", truncation=True, max_length=512, return_offsets_mapping=True)\n",
    "#         start_pos, end_pos = find_answer_positions(context, correct_answer)\n",
    "#         print(f\"start_pos, end_pos: {start_pos} {end_pos}\")\n",
    "\n",
    "#         if start_pos is None or end_pos is None:\n",
    "#             continue  # Skip if answer not found in tokenized context\n",
    "\n",
    "#         tokenized_inputs[\"input_ids\"].append(tokenized[\"input_ids\"])\n",
    "#         tokenized_inputs[\"attention_mask\"].append(tokenized[\"attention_mask\"])\n",
    "#         tokenized_inputs[\"start_positions\"].append(start_pos)\n",
    "#         tokenized_inputs[\"end_positions\"].append(end_pos)\n",
    "\n",
    "#     return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59508350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    \"\"\"\n",
    "    Extracts question, choices, correct answer, and tokenizes them.\n",
    "    \"\"\"\n",
    "    questions = examples[\"question\"]\n",
    "    choices = examples[\"choices\"]\n",
    "    answer_indices = examples[\"answer\"]\n",
    "    \n",
    "    answers_text = []\n",
    "    contexts = []\n",
    "    start_pos = []\n",
    "    end_pos = []\n",
    "    index = 0\n",
    "    qnc_offset = []\n",
    "    for e in examples[\"question\"]:\n",
    "        choices = examples[\"choices\"][index]\n",
    "        answer_index = examples[\"answer\"][index]\n",
    "        answer = choices[answer_index]\n",
    "        c =  f\"{e} Choices: {', '.join(choices)}\"\n",
    "        qnc_offset = len(f\"{e} Choices: ,\")\n",
    "        answers_text.append(answer)\n",
    "        contexts.append(c)\n",
    "        start_pos.append(qnc_offset+answer_index)\n",
    "        end_pos.append(qnc_offset+len(answer))\n",
    "        index+=1\n",
    "\n",
    "    # Extract correct answer text\n",
    "#     answers_text = [\n",
    "#         a[idx] if 0 <= idx < len(a) else \"\" for a, idx in zip(choices, answer_indices)\n",
    "#     ]\n",
    "\n",
    "    # Tokenize question\n",
    "    tokenized_questions = tokenizer(\n",
    "        contexts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Tokenize correct answer\n",
    "    tokenized_answers = tokenizer(\n",
    "        answers_text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    print(f'input sizes {len(tokenized_questions[\"input_ids\"])} {len(tokenized_answers[\"input_ids\"])}')\n",
    "\n",
    "    # Combine into a single dictionary\n",
    "    tokenized_inputs = {\n",
    "        \"input_ids\": tokenized_questions[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized_questions[\"attention_mask\"],\n",
    "        \"labels\": tokenized_answers[\"input_ids\"],  # Labels for causal language modeling\n",
    "        \"start_positions\":  start_pos,\n",
    "        \"end_positions\": end_pos\n",
    "    }\n",
    "\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66373b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "537169bb2a6b437b9a8366add86edee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12726 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input sizes 1000 1000\n",
      "input sizes 1000 1000\n",
      "input sizes 1000 1000\n",
      "input sizes 1000 1000\n",
      "input sizes 1000 1000\n",
      "input sizes 1000 1000\n",
      "input sizes 1000 1000\n",
      "input sizes 1000 1000\n",
      "input sizes 1000 1000\n",
      "input sizes 1000 1000\n",
      "input sizes 1000 1000\n",
      "input sizes 1000 1000\n",
      "input sizes 726 726\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "676f9d57b96a4e9e9f6c774c5a624e4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4241 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input sizes 1000 1000\n",
      "input sizes 1000 1000\n",
      "input sizes 1000 1000\n",
      "input sizes 1000 1000\n",
      "input sizes 241 241\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "# Convert to PyTorch dataset\n",
    "train_dataset = tokenized_dataset[\"train\"].with_format(\"torch\")\n",
    "valid_dataset = tokenized_dataset[\"validation\"].with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9fd84b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rckyi/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/Users/rckyi/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/transformers/training_args.py:1583: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./scienceqa_finetuned_lora\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=3e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    max_steps=1000,\n",
    "    no_cuda=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b40b687",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-2f13970cfe65>:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e09d3b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshaddie77\u001b[0m (\u001b[33mshaddie77-personal\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/rckyi/Documents/GitHub/LLM_FineTuning/wandb/run-20250214_101956-rhmc8rwb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shaddie77-personal/huggingface/runs/rhmc8rwb' target=\"_blank\">./scienceqa_finetuned_lora</a></strong> to <a href='https://wandb.ai/shaddie77-personal/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shaddie77-personal/huggingface' target=\"_blank\">https://wandb.ai/shaddie77-personal/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shaddie77-personal/huggingface/runs/rhmc8rwb' target=\"_blank\">https://wandb.ai/shaddie77-personal/huggingface/runs/rhmc8rwb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rckyi/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 43:55, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.483200</td>\n",
       "      <td>5.462069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=6.4942233276367185, metrics={'train_runtime': 2638.627, 'train_samples_per_second': 3.032, 'train_steps_per_second': 0.379, 'total_flos': 523537268736000.0, 'train_loss': 6.4942233276367185, 'epoch': 0.6285355122564424})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8867a976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning complete! Model saved at './scienceqa_finetuned_peft_lora_model'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save the fine-tuned model\n",
    "trainer.save_model(\"./scienceqa_finetuned_peft_lora_model\")\n",
    "tokenizer.save_pretrained(\"./scienceqa_finetuned_peft_lora_model\")\n",
    "\n",
    "print(\"Fine-tuning complete! Model saved at './scienceqa_finetuned_peft_lora_model'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1da34f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d25318dc09545de831e29c27bf5354c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86e8515a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rckyi/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/transformers/utils/hub.py:894: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92c55484527c44e4a858e11b36cfdd1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2bdd7e77ef347748c727368c3cfb5b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/302k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/shaddie/scienceqa_finetuned_peft_lora_model/commit/fe21929c0a1c0deb349795f310cd3a72b3cc4cf8', commit_message='fine-tuning-exercises', commit_description='', oid='fe21929c0a1c0deb349795f310cd3a72b3cc4cf8', pr_url=None, repo_url=RepoUrl('https://huggingface.co/shaddie/scienceqa_finetuned_peft_lora_model', endpoint='https://huggingface.co', repo_type='model', repo_id='shaddie/scienceqa_finetuned_peft_lora_model'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"shaddie/scienceqa_finetuned_peft_lora_model\",\n",
    "                  use_auth_token=True,\n",
    "                  commit_message=\"fine-tuning-exercises\",\n",
    "                  private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6bf9d8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0d901bb7ca04671be39da7e8e52f1dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/820 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "\n",
    "peft_model_id = \"shaddie/scienceqa_finetuned_peft_lora_model\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93fecb68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae10df36cf2f44399790f4f5664baa99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/302k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = PeftModel.from_pretrained(model, peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "01a665e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, choices):\n",
    "    \"\"\"\n",
    "    Given a science question, retrieves a context from ScienceQA and predicts the answer.\n",
    "    \"\"\"\n",
    "#     context = f\"{prompt_question} Choices: {', '.join(choices)}\"\n",
    "    c = ', '.join(choices)\n",
    "    context = f\"{prompt_question} Choices: \"+c \n",
    "    inputs = tokenizer(context, add_special_tokens=False)\n",
    "\n",
    "#     # Tokenize question with context\n",
    "#     inputs = tokenizer(question, context, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    \n",
    "    inputs = tokenizer(context, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "\n",
    "\n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    start_logits = outputs.start_logits\n",
    "    end_logits = outputs.end_logits\n",
    "\n",
    "    # Get the most probable start and end positions\n",
    "    start_index = torch.argmax(start_logits).item()\n",
    "    end_index = torch.argmax(end_logits).item()\n",
    "\n",
    "    # Ensure indices are within valid range\n",
    "    if start_index >= end_index or start_index <= 0 or end_index > len(inputs[\"input_ids\"][0]):\n",
    "        return \"Answer not found.\"\n",
    "\n",
    "    # Convert token IDs to words\n",
    "    answer_tokens = inputs[\"input_ids\"][0][start_index:end_index + 1]\n",
    "    answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
    "\n",
    "    return answer.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fde59ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the moon ' s gravitational pull\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_question = \"What causes tides?\"\n",
    "prompt_choices = [\"Tides are caused by the moon's gravitational pull.\", \"The sun also has an effect, but the moon is the dominant factor\" ]\n",
    "\n",
    "answer_question(prompt_question, prompt_choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556aff58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
