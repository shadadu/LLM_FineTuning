{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "850fa333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nFine tune a image captioning or description model on a artwork description data to\\nobtain a model that is able to analyze an artwork and generate a description of \\nthe artwork that is hopefully more art-review-like than a generic image captioning \\nmodel would\\n\\nThe pretrained model used here is the Salesforce Blip Captioniong model: \\nhttps://huggingface.co/Salesforce/blip-image-captioning-base\\n\\nThe fine tuning data set is the data from the SemArt Project: \\nhttps://github.com/noagarcia/SemArt\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "Fine tune a image captioning or description model on a artwork description data to\n",
    "obtain a model that is able to analyze an artwork and generate a description of \n",
    "the artwork that is hopefully more art-review-like than a generic image captioning \n",
    "model would\n",
    "\n",
    "The pretrained model used here is the Salesforce Blip Captioniong model: \n",
    "https://huggingface.co/Salesforce/blip-image-captioning-base\n",
    "\n",
    "The fine tuning data set is the data from the SemArt Project: \n",
    "https://github.com/noagarcia/SemArt\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a243aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import chardet\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import datasets\n",
    "from datasets import Dataset as HFDataset, DatasetDict, IterableDataset\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, ViTImageProcessor, VisionEncoderDecoderModel, AutoTokenizer, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2b19609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files_in_directory(dir_path):\n",
    "    try:\n",
    "        files = os.listdir(dir_path)\n",
    "        return [x for x in files if '.jpg' in x]\n",
    "    except FileNotFoundError:\n",
    "        return f\"Directory not found: {dir_path}\"\n",
    "    except NotADirectoryError:\n",
    "        return f\"Not a directory: {dir_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee848c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file path /Users/rckyi/Documents/Datasette/SemArtData/SemArt/semart_train.csv\n",
      "encoding utf-8\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Step 1: Load the CSV file as a dictionary mapping image names to descriptions\n",
    "semart_dir = f\"/Users/rckyi/Documents/Datasette/SemArtData/SemArt\" \n",
    "images_dir = semart_dir \n",
    "\n",
    "description_file_train = semart_dir + '/semart_train.csv'\n",
    "description_file_test = semart_dir + '/semart_test.csv'\n",
    "\n",
    "with open(description_file_train, 'rb') as file:\n",
    "    print(f'file path {description_file_train}')\n",
    "    result = chardet.detect(file.read())\n",
    "    encoding = result['encoding']\n",
    "    print(f'encoding {encoding}')\n",
    "    df_train = pd.read_csv(description_file_train, encoding=encoding, sep='\\t')\n",
    "    print(type(df_train['IMAGE_FILE'][0]))\n",
    "# df_test = pd.read_csv(description_file_test, encoding = \"utf-8\")\n",
    "\n",
    "\n",
    "image_to_description = dict(zip(df_train[\"IMAGE_FILE\"], df_train[\"DESCRIPTION\"]))\n",
    "# list(image_to_description.items())[0:5]\n",
    "\n",
    "# # Define the image directory and transformation\n",
    "# image_dir = \"path/to/images\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to a fixed shape\n",
    "    transforms.ToTensor(),  # Convert image to tensor\n",
    "#     transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb202927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Load the CSV file into a dictionary mapping image names to descriptions\n",
    "# description_file = \"path/to/description.csv\"\n",
    "# df = pd.read_csv(description_file)\n",
    "image_files_in_dir = list_files_in_directory(semart_dir+'/Images/')\n",
    "# image_files_in_dir\n",
    "# list(image_to_description.items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e023887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import IterableDataset, DatasetDict\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "def data_generator(images_name_list):\n",
    "    ret = []\n",
    "    image_file_list = list(df_train['IMAGE_FILE'])\n",
    "#     def generator():\n",
    "    for image_name in images_name_list:\n",
    "#         image_path = os.path.join(images_dir, image_name)\n",
    "        image_path = f\"{semart_dir}/Images/{image_name}\"\n",
    "\n",
    "        if image_name in image_file_list: #image_to_description: # and os.path.exists(image_path):\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            image_tensor = transform(image)\n",
    "#             ret.append({\"image\":image_tensor, \"text\":image_to_description[image_name]})\n",
    "            yield {\"image\":image_tensor, \"text\":image_to_description[image_name]}\n",
    "#     return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfd61c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_var = data_generator(image_files_in_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5387957b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dee676e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'generator' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m hf_dataset \u001b[38;5;241m=\u001b[39m DatasetDict({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_var\u001b[49m\u001b[43m)\u001b[49m})\n\u001b[1;32m      2\u001b[0m hf_dataset\n",
      "File \u001b[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/datasets/arrow_dataset.py:970\u001b[0m, in \u001b[0;36mDataset.from_list\u001b[0;34m(cls, mapping, features, info, split)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;124;03mConvert a list of dicts to a `pyarrow.Table` to create a [`Dataset`]`.\u001b[39;00m\n\u001b[1;32m    956\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;124;03m    [`Dataset`]\u001b[39;00m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;66;03m# for simplicity and consistency wrt OptimizedTypedSequence we do not use InMemoryTable.from_pylist here\u001b[39;00m\n\u001b[0;32m--> 970\u001b[0m mapping \u001b[38;5;241m=\u001b[39m {k: [r\u001b[38;5;241m.\u001b[39mget(k) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m mapping] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m} \u001b[38;5;28;01mif\u001b[39;00m mapping \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    971\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_dict(mapping, features, info, split)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'generator' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "hf_dataset = DatasetDict({\"train\": datasets.Dataset.from_list(data_var)})\n",
    "hf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcf965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(example_batch):\n",
    "#     ((V + 1.0) * 255/2).astype(np.uint8)\n",
    "    images = np.asarray([x for x in example_batch[\"image\"]])\n",
    "    captions = [x for x in example_batch[\"text\"]]\n",
    "    inputs = processor(images=images, text=captions, padding=\"max_length\")\n",
    "    inputs.update({\"labels\": inputs[\"input_ids\"]})\n",
    "    return inputs\n",
    "\n",
    "hf_dataset.set_transform(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b29763",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49ae25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom collator to handle the format expected by the BLIP model\n",
    "def collate_fn(batch):\n",
    "#     print(f'batch {batch}')\n",
    "    images = [item['image'] for item in batch]\n",
    "    texts = [item['text'] for item in batch]\n",
    "    # Convert the images into tensor and normalize\n",
    "    images = torch.stack(images)\n",
    "    images = torch.add(images,1.0)\n",
    "    text = torch.stack(texts)\n",
    "    return processor(images=images, text=texts, padding=True, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee13099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./blip-finetuned\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    save_steps=500,\n",
    "    evaluation_strategy=None,\n",
    "#     evaluation_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    push_to_hub=False,  # Set to True if uploading to Hugging Face Hub\n",
    "    logging_steps=10,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    remove_unused_columns=False,\n",
    "    max_steps=10,\n",
    "    no_cuda=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbab7524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pretrained BLIP Model & Processor\n",
    "model_name = \"Salesforce/blip-image-captioning-base\"\n",
    "processor = BlipProcessor.from_pretrained(model_name)\n",
    "model = BlipForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f95ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=hf_dataset[\"train\"].with_format(\"torch\"),\n",
    "#     tokenizer=processor,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271cda9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset= hf_dataset[\"train\"],\n",
    "    tokenizer=processor,\n",
    "#     data_collator=collate_fn,  # Use custom collate function to prepare batches\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499864f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4547a655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Fine-Tuning\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
