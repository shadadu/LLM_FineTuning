{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "850fa333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nFine tune a image captioning or description model on a artwork description data to\\nobtain a model that is able to analyze an artwork and generate a description of \\nthe artwork that is hopefully more art-review-like than a generic image captioning \\nmodel would\\n\\nThe pretrained model used here is the Salesforce Blip Captioniong model: \\nhttps://huggingface.co/Salesforce/blip-image-captioning-base\\n\\nThe fine tuning data set is the data from the SemArt Project: \\nhttps://github.com/noagarcia/SemArt\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "Fine tune a image captioning or description model on a artwork description data to\n",
    "obtain a model that is able to analyze an artwork and generate a description of \n",
    "the artwork that is hopefully more art-review-like than a generic image captioning \n",
    "model would\n",
    "\n",
    "The pretrained model used here is the Salesforce Blip Captioniong model: \n",
    "https://huggingface.co/Salesforce/blip-image-captioning-base\n",
    "\n",
    "The fine tuning data set is the data from the SemArt Project: \n",
    "https://github.com/noagarcia/SemArt\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a243aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import chardet\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import datasets\n",
    "from datasets import Dataset as HFDataset, DatasetDict, IterableDataset\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, ViTImageProcessor, VisionEncoderDecoderModel, AutoTokenizer, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2b19609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files_in_directory(dir_path):\n",
    "    try:\n",
    "        files = os.listdir(dir_path)\n",
    "        return [x for x in files if '.jpg' in x]\n",
    "    except FileNotFoundError:\n",
    "        return f\"Directory not found: {dir_path}\"\n",
    "    except NotADirectoryError:\n",
    "        return f\"Not a directory: {dir_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee848c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file path /Users/rckyi/Documents/Datasette/SemArtData/SemArt/semart_train.csv\n",
      "encoding utf-8\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Step 1: Load the CSV file as a dictionary mapping image names to descriptions\n",
    "semart_dir = f\"/Users/rckyi/Documents/Datasette/SemArtData/SemArt\" \n",
    "images_dir = semart_dir \n",
    "\n",
    "description_file_train = semart_dir + '/semart_train.csv'\n",
    "description_file_test = semart_dir + '/semart_test.csv'\n",
    "\n",
    "with open(description_file_train, 'rb') as file:\n",
    "    print(f'file path {description_file_train}')\n",
    "    result = chardet.detect(file.read())\n",
    "    encoding = result['encoding']\n",
    "    print(f'encoding {encoding}')\n",
    "    df_train = pd.read_csv(description_file_train, encoding=encoding, sep='\\t')\n",
    "    print(type(df_train['IMAGE_FILE'][0]))\n",
    "# df_test = pd.read_csv(description_file_test, encoding = \"utf-8\")\n",
    "\n",
    "\n",
    "image_to_description = dict(zip(df_train[\"IMAGE_FILE\"], df_train[\"DESCRIPTION\"]))\n",
    "# list(image_to_description.items())[0:5]\n",
    "\n",
    "# # Define the image directory and transformation\n",
    "# image_dir = \"path/to/images\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to a fixed shape\n",
    "    transforms.ToTensor(),  # Convert image to tensor\n",
    "#     transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb202927",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files_in_dir = list_files_in_directory(semart_dir+'/Images/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e023887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import IterableDataset, DatasetDict\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "def data_generator():\n",
    "    image_file_list = list(df_train['IMAGE_FILE'])\n",
    "    for image_name in image_files_in_dir:\n",
    "        image_path = f\"{semart_dir}/Images/{image_name}\"\n",
    "\n",
    "        if image_name in image_file_list: #image_to_description: # and os.path.exists(image_path):\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            image_tensor = transform(image))\n",
    "            yield {\"image\":image_tensor, \"text\":image_to_description[image_name]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dee676e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d074b1394e4ac6b9595f594cfec303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'text'],\n",
       "        num_rows: 19244\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset = DatasetDict({\"train\": datasets.Dataset.from_generator(data_generator)})\n",
    "hf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbcf965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(example_batch):\n",
    "    images = np.asarray([x for x in example_batch[\"image\"]])\n",
    "    captions = [x for x in example_batch[\"text\"]]\n",
    "    inputs = processor(images=images, text=captions, \n",
    "                       padding=\"max_length\",   # âœ… Pads or truncates to max_length\n",
    "                        truncation=True,        # âœ… Prevents sequences longer than max_length\n",
    "                        max_length=512  )\n",
    "    inputs.update({\"labels\": inputs[\"input_ids\"]})\n",
    "    return inputs\n",
    "\n",
    "hf_dataset.set_transform(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e49ae25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a custom collator to handle the format expected by the BLIP model\n",
    "# def collate_fn(batch):\n",
    "#     images = [item['image'] for item in batch]\n",
    "#     texts = [item['text'] for item in batch]\n",
    "#     # Convert the images into tensor and normalize\n",
    "#     images = torch.stack(images)\n",
    "#     images = torch.add(images,1.0)\n",
    "#     text = torch.stack(texts)\n",
    "#     return processor(images=images, text=texts, padding=True, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ee13099",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rckyi/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/transformers/training_args.py:1583: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ðŸ¤— Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./blip-semart-finetuned\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    save_steps=500,\n",
    "    evaluation_strategy=None,\n",
    "#     evaluation_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    push_to_hub=False,  # Set to True if uploading to Hugging Face Hub\n",
    "    logging_steps=10,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    remove_unused_columns=False,\n",
    "    max_steps=1000,\n",
    "    no_cuda=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbab7524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pretrained BLIP Model & Processor\n",
    "model_name = \"Salesforce/blip-image-captioning-base\"\n",
    "processor = BlipProcessor.from_pretrained(model_name)\n",
    "model = BlipForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "271cda9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-08206ab134dc>:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset= hf_dataset[\"train\"],\n",
    "    tokenizer=processor,\n",
    "#     data_collator=collate_fn,  # Use custom collate function to prepare batches\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4547a655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshaddie77\u001b[0m (\u001b[33mshaddie77-personal\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/rckyi/Documents/GitHub/LLM_FineTuning/wandb/run-20250219_041046-qz4uk8fx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shaddie77-personal/huggingface/runs/qz4uk8fx' target=\"_blank\">./blip-semart-finetuned</a></strong> to <a href='https://wandb.ai/shaddie77-personal/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shaddie77-personal/huggingface' target=\"_blank\">https://wandb.ai/shaddie77-personal/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shaddie77-personal/huggingface/runs/qz4uk8fx' target=\"_blank\">https://wandb.ai/shaddie77-personal/huggingface/runs/qz4uk8fx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='286' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 286/1000 8:19:27 < 20:55:41, 0.01 it/s, Epoch 0.12/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>9.368300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>7.280400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>6.327100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>5.402500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.447900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.629100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.583100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.931700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.724900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.329500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.213000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.281400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.158300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.150500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.415000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.267100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.202700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.386100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.341900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.410000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.093500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.174200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.048900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.067300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.204400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.091700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.310700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.047900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start Fine-Tuning\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de70e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "trainer.save_model(\"./blip-semart-finetuned\")\n",
    "tokenizer.save_pretrained(\"./blip-semart-finetuned\")\n",
    "\n",
    "print(\"Fine-tuning complete! Model saved at './blip-semart-finetuned'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9558d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_files_in_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4613004e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import requests\n",
    "\n",
    "# im_path = semart_dir+'/Images/'+ '14152-08abbe.jpg'\n",
    "# image = Image.open(im_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e9c76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from accelerate.test_utils.testing import get_backend\n",
    "# \n",
    "# device, _, _ = get_backend()\n",
    "# inputs = processor(images=image, return_tensors=\"pt\").to(torch.float).to(device)\n",
    "# pixel_values = inputs.pixel_values.to(torch.float)\n",
    "# model = model.to(device)\n",
    "# device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155002b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated_ids = model.generate(pixel_values=pixel_values, max_length=50)\n",
    "# generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "# print(generated_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f450ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
