{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb055ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Fine tune a image captioning or description model on a artwork description data to\n",
    "obtain a model that is able to analyze an artwork and generate a description of \n",
    "the artwork that is hopefully more art-review-like than a generic image captioning \n",
    "model would\n",
    "\n",
    "The pretrained model used here is the Salesforce Blip Captioniong model: \n",
    "https://huggingface.co/Salesforce/blip-image-captioning-base\n",
    "\n",
    "The fine tuning data set is the data from the SemArt Project: \n",
    "https://github.com/noagarcia/SemArt\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af97015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import chardet\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import Dataset as HFDataset, DatasetDict, IterableDataset\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, ViTImageProcessor, VisionEncoderDecoderModel, AutoTokenizer, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937dac2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files_in_directory(dir_path):\n",
    "    try:\n",
    "        files = os.listdir(dir_path)\n",
    "        return [x for x in files if '.jpg' in x]\n",
    "    except FileNotFoundError:\n",
    "        return f\"Directory not found: {dir_path}\"\n",
    "    except NotADirectoryError:\n",
    "        return f\"Not a directory: {dir_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7f7e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Step 1: Load the CSV file as a dictionary mapping image names to descriptions\n",
    "semart_dir = f\"/Users/rckyi/Documents/Datasette/SemArtData/SemArt\" \n",
    "images_dir = semart_dir \n",
    "\n",
    "description_file_train = semart_dir + '/semart_train.csv'\n",
    "description_file_test = semart_dir + '/semart_test.csv'\n",
    "\n",
    "with open(description_file_train, 'rb') as file:\n",
    "    print(f'file path {description_file_train}')\n",
    "    result = chardet.detect(file.read())\n",
    "    encoding = result['encoding']\n",
    "    print(f'encoding {encoding}')\n",
    "    df_train = pd.read_csv(description_file_train, encoding=encoding, sep='\\t')\n",
    "    print(type(df_train['IMAGE_FILE'][0]))\n",
    "# df_test = pd.read_csv(description_file_test, encoding = \"utf-8\")\n",
    "\n",
    "\n",
    "image_to_description = dict(zip(df_train[\"IMAGE_FILE\"], df_train[\"DESCRIPTION\"]))\n",
    "# list(image_to_description.items())[0:5]\n",
    "\n",
    "# # Define the image directory and transformation\n",
    "# image_dir = \"path/to/images\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to a fixed shape\n",
    "    transforms.ToTensor(),  # Convert image to tensor\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize\n",
    "])\n",
    "\n",
    "# Step 2: Define a generator to load images and descriptions lazily\n",
    "# def data_generator(images_name_list):\n",
    "#     for image_name in images_name_list:\n",
    "#         image_path = os.path.join(image_dir, image_name)\n",
    "\n",
    "#         if image_name in image_to_description and os.path.exists(image_path):\n",
    "#             # Load and transform the image\n",
    "#             image = Image.open(image_path).convert(\"RGB\")\n",
    "#             image_tensor = transform(image)\n",
    "\n",
    "#             # Yield image name, image tensor, and description\n",
    "#             yield image_name, image_tensor, image_to_description[image_name]\n",
    "\n",
    "# # Step 3: Iterate through the generator and build the dictionary\n",
    "# data_var = {img_name: [img_tensor, desc] for img_name, img_tensor, desc in data_generator([\"image1.jpg\", \"image2.jpg\", \"image3.jpg\"])}\n",
    "\n",
    "# # Print a sample entry\n",
    "# print(next(iter(data_var.items())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d78ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Load the CSV file into a dictionary mapping image names to descriptions\n",
    "# description_file = \"path/to/description.csv\"\n",
    "# df = pd.read_csv(description_file)\n",
    "image_files_in_dir = list_files_in_directory(semart_dir+'/Images/')\n",
    "# image_files_in_dir\n",
    "# list(image_to_description.items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e180f335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(images_name_list):\n",
    "    image_file_list = list(df_train['IMAGE_FILE'])\n",
    "    for image_name in images_name_list:\n",
    "#         image_path = os.path.join(images_dir, image_name)\n",
    "        image_path = f\"{semart_dir}/Images/{image_name}\"\n",
    "\n",
    "        if image_name in image_file_list: #image_to_description: # and os.path.exists(image_path):\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            image_tensor = transform(image)\n",
    "            yield image_name, image_tensor, image_to_description[image_name]\n",
    "\n",
    "# Step 3: Iterate through the generator and build the dictionary\n",
    "data_var = {img_name: [img_tensor, desc] for img_name, img_tensor, desc in data_generator(image_files_in_dir)}\n",
    "\n",
    "# # Print a sample entry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab69279b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, b in data_var.items():\n",
    "#     print(f'i {i}, b {b}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0958e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import IterableDataset, DatasetDict\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "def convert_dict_to_hf_dataset(data_dict):\n",
    "    \"\"\"\n",
    "    Converts a dictionary {img_name: [img_tensor, desc]} into Hugging Face Dataset format.\n",
    "    Uses a generator for efficient streaming.\n",
    "    \"\"\"\n",
    "    def data_generator():\n",
    "        for _, (img_tensor, desc) in data_dict.items():  # Ignore img_name\n",
    "            yield {\"image\": img_tensor, \"text\": desc}\n",
    "    \n",
    "    # Use IterableDataset.from_generator() for streaming support\n",
    "    hf_dataset = IterableDataset.from_generator(data_generator)\n",
    "    return DatasetDict({\"train\": hf_dataset})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c6869e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert dictionary to Hugging Face Dataset format\n",
    "hf_dataset = convert_dict_to_hf_dataset(data_var)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddb7b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./blip-finetuned\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    save_steps=500,\n",
    "    evaluation_strategy=None,\n",
    "#     evaluation_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    push_to_hub=False,  # Set to True if uploading to Hugging Face Hub\n",
    "    logging_steps=10,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    max_steps=10,\n",
    "    no_cuda=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7256b61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pretrained BLIP Model & Processor\n",
    "model_name = \"Salesforce/blip-image-captioning-base\"\n",
    "processor = BlipProcessor.from_pretrained(model_name)\n",
    "model = BlipForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0893e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=hf_dataset[\"train\"],\n",
    "    tokenizer=processor,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2c8534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Fine-Tuning\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d9b070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert the DataFrame into a dictionary {image_name: description}\n",
    "# image_to_description = dict(zip(df_train[\"IMAGE_FILE\"], df_train[\"DESCRIPTION\"]))\n",
    "\n",
    "# # Step 2: Iterate through the list of image names and get corresponding descriptions\n",
    "# # images_name_list = [\"image1.jpg\", \"image2.jpg\", \"image3.jpg\"]  # Example list of image names\n",
    "\n",
    "# Define a transformation to convert images to PyTorch tensors\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to a fixed shape\n",
    "    transforms.ToTensor(),  # Convert image to tensor\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize\n",
    "])\n",
    "\n",
    "# # Step 3 & 4: Load each image as a tensor and create the dictionary data_var\n",
    "# data_var = {}\n",
    "\n",
    "# for image_name in image_files_in_dir:\n",
    "#     image_path = f\"{semart_dir}/Images/{image_name}\"  # Update with the correct path\n",
    "\n",
    "#     if image_name in image_to_description:  # Ensure image exists in CSV\n",
    "#         # Load and transform the image\n",
    "#         image = Image.open(image_path).convert(\"RGB\")\n",
    "#         image_tensor = transform(image)\n",
    "\n",
    "#         # Get the corresponding description\n",
    "#         description = image_to_description[image_name]\n",
    "\n",
    "#         # Store in dictionary\n",
    "#         data_var[image_name] = [image_tensor, description]\n",
    "\n",
    "# # Now, `data_var` contains image tensors mapped to their descriptions\n",
    "# print(data_var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a8a25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define Custom PyTorch Dataset\n",
    "# class ImageDescriptionDataset(Dataset):\n",
    "#     def __init__(self, image_names, image_dir, transform):\n",
    "#         self.image_names = image_names\n",
    "#         self.image_dir = image_dir\n",
    "#         self.transform = transform\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.image_names)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         image_name = self.image_names[idx]\n",
    "#         image_path = os.path.join(self.image_dir, image_name)\n",
    "\n",
    "#         if os.path.exists(image_path) and image_name in image_to_description:\n",
    "#             # Load & transform image\n",
    "#             image = Image.open(image_path).convert(\"RGB\")\n",
    "#             image_tensor = self.transform(image)\n",
    "\n",
    "#             # Get text description\n",
    "#             description = image_to_description[image_name]\n",
    "\n",
    "#             return {\"image\": image_tensor, \"text\": description}\n",
    "#         else:\n",
    "#             return {\"image\": torch.zeros((3, 224, 224)), \"text\": \"\"}  # Return empty if missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d0487a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDescriptionDataset(Dataset):\n",
    "    def __init__(self, image_names, image_dir, transform):\n",
    "        self.image_names = image_names\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_names[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_name)\n",
    "\n",
    "        if os.path.exists(image_path) and image_name in image_to_description:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            image_tensor = self.transform(image)\n",
    "            description = image_to_description[image_name]\n",
    "            return {\"image\": image_tensor, \"text\": description}\n",
    "        else:\n",
    "            return {\"image\": torch.zeros((3, 384, 384)), \"text\": \"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed89620f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load Data with DataLoader (Streaming)\n",
    "\n",
    "\n",
    "# images_name_list = df_train[\"IMAGE_FILE\"].tolist()  # Get list of image names from CSV\n",
    "# dataset_train = ImageDescriptionDataset(image_files_in_dir, images_dir, transform)\n",
    "\n",
    "# # Wrap in DataLoader for efficient streaming\n",
    "# data_loader_train = DataLoader(dataset_train, batch_size=8, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a631dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Convert DataLoader to Hugging Face Dataset\n",
    "# def convert_to_hf_format(dataloader):\n",
    "#     \"\"\"Convert PyTorch DataLoader output into Hugging Face Dataset format\"\"\"\n",
    "#     image_list, text_list = [], []\n",
    "    \n",
    "#     for batch in dataloader:\n",
    "#         image_list.extend(batch[\"image\"])\n",
    "#         text_list.extend(batch[\"text\"])\n",
    "\n",
    "#     # Convert to Hugging Face Dataset\n",
    "#     return HFDataset.from_dict({\"image\": image_list, \"text\": text_list})\n",
    "\n",
    "# hf_dataset = convert_to_hf_format(data_loader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa4a9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Convert DataLoader to Hugging Face Streaming Dataset\n",
    "def convert_to_hf_format(dataloader):\n",
    "    \"\"\"Generator-based function to convert DataLoader output into a Hugging Face streaming dataset.\"\"\"\n",
    "    \n",
    "    def data_generator():\n",
    "        for batch in dataloader:\n",
    "            for image_tensor, text in zip(batch[\"image\"], batch[\"text\"]):\n",
    "                yield {\"image\": image_tensor, \"text\": text}\n",
    "\n",
    "    # Create an IterableDataset for Hugging Face Trainer\n",
    "    return IterableDataset.from_generator(data_generator)\n",
    "\n",
    "# Convert PyTorch DataLoader to Hugging Face Streaming Dataset\n",
    "hf_dataset_train = convert_to_hf_format(data_loader_train)\n",
    "\n",
    "# Wrap in DatasetDict for Hugging Face Trainer\n",
    "dataset_dict = DatasetDict({\"train\": hf_dataset_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6200615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 4: Load Pretrained BLIP Model & Processor\n",
    "# model_name = \"Salesforce/blip-image-captioning-base\"\n",
    "# processor = BlipProcessor.from_pretrained(model_name)\n",
    "# model = BlipForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febc9236",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ydshieh/vit-gpt2-coco-en\"\n",
    "model = VisionEncoderDecoderModel.from_pretrained(model_name)\n",
    "processor = ViTImageProcessor.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450495c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b122d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if tokenizer.pad_token is None:\n",
    "#     tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "#     model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a745f34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./blip-finetuned-semart\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    save_strategy=\"epoch\",\n",
    "#     save_steps=500,\n",
    "    evaluation_strategy=None,\n",
    "#     evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=1,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    max_steps=5,\n",
    "#     no_cuda=True\n",
    ")\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./scienceqa_finetuned_lora\",\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     per_device_train_batch_size=8,\n",
    "#     per_device_eval_batch_size=8,\n",
    "#     learning_rate=3e-5,\n",
    "#     num_train_epochs=3,\n",
    "#     weight_decay=0.01,\n",
    "#     save_total_limit=2,\n",
    "#     logging_dir=\"./logs\",\n",
    "#     logging_steps=10,\n",
    "#     fp16=torch.cuda.is_available(),\n",
    "#     max_steps=1000,\n",
    "#     no_cuda=True\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a5208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Fine-tune the Model with Hugging Face Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_dict[\"train\"],\n",
    "#     eval_dataset=dataset_dict[\"train\"],\n",
    "    tokenizer=processor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f00dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d500330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Fine-tuned Model\n",
    "model.save_pretrained(\"./blip-finetuned-semart\")\n",
    "processor.save_pretrained(\"./blip-finetuned-semart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e56b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Set training configurations\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./blip-finetuned-semart\",\n",
    "#     per_device_train_batch_size=8,\n",
    "#     num_train_epochs=5,\n",
    "#     save_steps=100,\n",
    "#     save_total_limit=2,\n",
    "#     logging_dir=\"./logs\",\n",
    "#     logging_steps=50,\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     eval_steps=500,\n",
    "#     learning_rate=5e-5,\n",
    "#     weight_decay=0.01,\n",
    "#     fp16=True,\n",
    "#     report_to=\"none\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da32ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Fine-tune the BLIP model\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=hf_dataset,\n",
    "#     tokenizer=processor\n",
    "# )\n",
    "\n",
    "# trainer.train()\n",
    "\n",
    "# # Save the fine-tuned model\n",
    "# model.save_pretrained(\"./blip-finetuned-semart\")\n",
    "# processor.save_pretrained(\"./blip-finetuned-semart\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
