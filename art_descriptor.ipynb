{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "850fa333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nFine tune a image captioning or description model on a artwork description data to\\nobtain a model that is able to analyze an artwork and generate a description of \\nthe artwork that is hopefully more art-review-like than a generic image captioning \\nmodel would\\n\\nThe pretrained model used here is the Salesforce Blip Captioniong model: \\nhttps://huggingface.co/Salesforce/blip-image-captioning-base\\n\\nThe fine tuning data set is the data from the SemArt Project: \\nhttps://github.com/noagarcia/SemArt\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "Fine tune a image captioning or description model on a artwork description data to\n",
    "obtain a model that is able to analyze an artwork and generate a description of \n",
    "the artwork that is hopefully more art-review-like than a generic image captioning \n",
    "model would\n",
    "\n",
    "The pretrained model used here is the Salesforce Blip Captioniong model: \n",
    "https://huggingface.co/Salesforce/blip-image-captioning-base\n",
    "\n",
    "The fine tuning data set is the data from the SemArt Project: \n",
    "https://github.com/noagarcia/SemArt\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a243aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import chardet\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import datasets\n",
    "from datasets import Dataset as HFDataset, DatasetDict, IterableDataset\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, ViTImageProcessor, VisionEncoderDecoderModel, AutoTokenizer, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2b19609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files_in_directory(dir_path):\n",
    "    try:\n",
    "        files = os.listdir(dir_path)\n",
    "        return [x for x in files if '.jpg' in x]\n",
    "    except FileNotFoundError:\n",
    "        return f\"Directory not found: {dir_path}\"\n",
    "    except NotADirectoryError:\n",
    "        return f\"Not a directory: {dir_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee848c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file path /Users/rckyi/Documents/Datasette/SemArtData/SemArt/semart_train.csv\n",
      "encoding utf-8\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Step 1: Load the CSV file as a dictionary mapping image names to descriptions\n",
    "semart_dir = f\"/Users/rckyi/Documents/Datasette/SemArtData/SemArt\" \n",
    "images_dir = semart_dir \n",
    "\n",
    "description_file_train = semart_dir + '/semart_train.csv'\n",
    "description_file_test = semart_dir + '/semart_test.csv'\n",
    "\n",
    "with open(description_file_train, 'rb') as file:\n",
    "    print(f'file path {description_file_train}')\n",
    "    result = chardet.detect(file.read())\n",
    "    encoding = result['encoding']\n",
    "    print(f'encoding {encoding}')\n",
    "    df_train = pd.read_csv(description_file_train, encoding=encoding, sep='\\t')\n",
    "    print(type(df_train['IMAGE_FILE'][0]))\n",
    "# df_test = pd.read_csv(description_file_test, encoding = \"utf-8\")\n",
    "\n",
    "\n",
    "image_to_description = dict(zip(df_train[\"IMAGE_FILE\"], df_train[\"DESCRIPTION\"]))\n",
    "# list(image_to_description.items())[0:5]\n",
    "\n",
    "# # Define the image directory and transformation\n",
    "# image_dir = \"path/to/images\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to a fixed shape\n",
    "    transforms.ToTensor(),  # Convert image to tensor\n",
    "#     transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb202927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Load the CSV file into a dictionary mapping image names to descriptions\n",
    "# description_file = \"path/to/description.csv\"\n",
    "# df = pd.read_csv(description_file)\n",
    "image_files_in_dir = list_files_in_directory(semart_dir+'/Images/')\n",
    "# image_files_in_dir\n",
    "# list(image_to_description.items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47e24dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def data_generator(images_name_list):\n",
    "#     image_file_list = list(df_train['IMAGE_FILE'])\n",
    "#     for image_name in images_name_list:\n",
    "# #         image_path = os.path.join(images_dir, image_name)\n",
    "#         image_path = f\"{semart_dir}/Images/{image_name}\"\n",
    "\n",
    "#         if image_name in image_file_list: #image_to_description: # and os.path.exists(image_path):\n",
    "#             image = Image.open(image_path).convert(\"RGB\")\n",
    "#             image_tensor = transform(image)\n",
    "#             yield image_name, image_tensor, image_to_description[image_name]\n",
    "\n",
    "# # Step 3: Iterate through the generator and build the dictionary\n",
    "# data_var = {img_name: [img_tensor, desc] for img_name, img_tensor, desc in data_generator(image_files_in_dir)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e023887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import IterableDataset, DatasetDict\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "def data_generator(images_name_list):\n",
    "    ret = []\n",
    "    image_file_list = list(df_train['IMAGE_FILE'])\n",
    "#     def generator():\n",
    "    for image_name in images_name_list:\n",
    "#         image_path = os.path.join(images_dir, image_name)\n",
    "        image_path = f\"{semart_dir}/Images/{image_name}\"\n",
    "\n",
    "        if image_name in image_file_list: #image_to_description: # and os.path.exists(image_path):\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            image_tensor = transform(image)\n",
    "            ret.append({\"image\":image_tensor, \"text\":image_to_description[image_name]})\n",
    "#             yield {\"image\":image_tensor, \"text\":image_to_description[image_name]}\n",
    "    return ret\n",
    "\n",
    "        \n",
    "#     # Use IterableDataset.from_generator() for streaming support\n",
    "#     hfd = IterableDataset.from_generator(data_generator)\n",
    "#     return hfd # DatasetDict({\"train\": hfd})\n",
    "\n",
    "# Step 3: Iterate through the generator and build the dictionary\n",
    "# y = data_generator(images_name_list=image_files_in_dir)\n",
    "# data_var = []\n",
    "# for x in data_generator(images_name_list=image_files_in_dir):\n",
    "#     data_var.append({\"image\": x[\"image\"], \"text\": x[\"text\"]} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfd61c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_var = data_generator(image_files_in_dir[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5387957b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'image': tensor([[[0.0392, 0.0078, 0.0039,  ..., 0.0549, 0.0510, 0.0784],\n",
      "         [0.1333, 0.0471, 0.0275,  ..., 0.1216, 0.0784, 0.0824],\n",
      "         [0.1412, 0.1176, 0.1569,  ..., 0.0745, 0.0745, 0.0902],\n",
      "         ...,\n",
      "         [0.2941, 0.2627, 0.1922,  ..., 0.2078, 0.1569, 0.1961],\n",
      "         [0.2863, 0.2588, 0.2039,  ..., 0.2510, 0.2196, 0.2314],\n",
      "         [0.2392, 0.2510, 0.2275,  ..., 0.3255, 0.3059, 0.3216]],\n",
      "\n",
      "        [[0.0431, 0.0157, 0.0078,  ..., 0.0902, 0.0863, 0.1137],\n",
      "         [0.1294, 0.0431, 0.0235,  ..., 0.1569, 0.1137, 0.1176],\n",
      "         [0.1255, 0.1020, 0.1412,  ..., 0.1098, 0.1098, 0.1255],\n",
      "         ...,\n",
      "         [0.2118, 0.1843, 0.1255,  ..., 0.1804, 0.1373, 0.1686],\n",
      "         [0.1922, 0.1725, 0.1255,  ..., 0.2039, 0.1922, 0.2039],\n",
      "         [0.1608, 0.1765, 0.1569,  ..., 0.2706, 0.2667, 0.2745]],\n",
      "\n",
      "        [[0.0235, 0.0039, 0.0039,  ..., 0.0235, 0.0196, 0.0471],\n",
      "         [0.0980, 0.0235, 0.0157,  ..., 0.0902, 0.0510, 0.0510],\n",
      "         [0.0863, 0.0667, 0.1176,  ..., 0.0471, 0.0431, 0.0588],\n",
      "         ...,\n",
      "         [0.1608, 0.1373, 0.0863,  ..., 0.0902, 0.0667, 0.1020],\n",
      "         [0.1412, 0.1294, 0.0902,  ..., 0.1059, 0.1216, 0.1373],\n",
      "         [0.1059, 0.1255, 0.1059,  ..., 0.1529, 0.1686, 0.1804]]]), 'text': 'This is a workshop copy of the official portrait Titian painted for the Hall of Greater Council in the Palazzo Ducale in c. 1537. All paintings in the Hall were destroyed in 1577 in the second terrible fire of the decade. This official image is much tamer than the later portrait of c. 1545 - painted some seven years after the death of Gritti - which was conceived along much less formal lines, consequently conveying a sense of the larger-than-life personality of the most effective Venetian doge of the sixteenth century'}, {'image': tensor([[[0.2392, 0.2078, 0.2078,  ..., 0.2157, 0.2000, 0.2000],\n",
      "         [0.2353, 0.2157, 0.2078,  ..., 0.2039, 0.1961, 0.2000],\n",
      "         [0.2314, 0.2275, 0.2157,  ..., 0.2039, 0.1961, 0.1922],\n",
      "         ...,\n",
      "         [0.2588, 0.2824, 0.3294,  ..., 0.2902, 0.3020, 0.3020],\n",
      "         [0.2235, 0.2275, 0.2275,  ..., 0.3020, 0.3098, 0.3020],\n",
      "         [0.2784, 0.2627, 0.2549,  ..., 0.3176, 0.3412, 0.3098]],\n",
      "\n",
      "        [[0.2000, 0.1686, 0.1686,  ..., 0.1725, 0.1529, 0.1529],\n",
      "         [0.1961, 0.1765, 0.1686,  ..., 0.1608, 0.1529, 0.1529],\n",
      "         [0.1922, 0.1882, 0.1765,  ..., 0.1647, 0.1569, 0.1529],\n",
      "         ...,\n",
      "         [0.2157, 0.2314, 0.2627,  ..., 0.2078, 0.2118, 0.2157],\n",
      "         [0.2196, 0.2157, 0.2118,  ..., 0.2196, 0.2196, 0.2157],\n",
      "         [0.2471, 0.2275, 0.2235,  ..., 0.2392, 0.2510, 0.2275]],\n",
      "\n",
      "        [[0.1922, 0.1608, 0.1608,  ..., 0.1686, 0.1529, 0.1529],\n",
      "         [0.1882, 0.1686, 0.1608,  ..., 0.1569, 0.1490, 0.1529],\n",
      "         [0.1843, 0.1804, 0.1686,  ..., 0.1608, 0.1529, 0.1490],\n",
      "         ...,\n",
      "         [0.1961, 0.2157, 0.2510,  ..., 0.1882, 0.2039, 0.2157],\n",
      "         [0.2039, 0.2039, 0.2000,  ..., 0.2000, 0.2078, 0.2078],\n",
      "         [0.2431, 0.2275, 0.2235,  ..., 0.2118, 0.2314, 0.2157]]]), 'text': 'Benjamin Gerritsz Cuyp belonged to the Cuyp family of Dutch painters. He was the half-brother of Jacob Gerritsz. Cuyp, father of the most famous member of the family, Aelbert Cuyp. Benjamin is noted principally for paintings of biblical and genre scenes which use \"Rembrandtesque\" light and shadow effects.'}, {'image': tensor([[[0.0471, 0.0431, 0.0392,  ..., 0.7490, 0.6941, 0.5608],\n",
      "         [0.0392, 0.0275, 0.0275,  ..., 0.7412, 0.8353, 0.6824],\n",
      "         [0.0314, 0.0392, 0.0157,  ..., 0.7608, 0.8745, 0.7647],\n",
      "         ...,\n",
      "         [0.0471, 0.0353, 0.0392,  ..., 0.1647, 0.1882, 0.1961],\n",
      "         [0.0627, 0.0314, 0.0431,  ..., 0.2392, 0.2745, 0.2745],\n",
      "         [0.0588, 0.0314, 0.0902,  ..., 0.4078, 0.2863, 0.2745]],\n",
      "\n",
      "        [[0.0392, 0.0353, 0.0314,  ..., 0.6980, 0.6510, 0.5137],\n",
      "         [0.0314, 0.0196, 0.0196,  ..., 0.6902, 0.7882, 0.6392],\n",
      "         [0.0235, 0.0314, 0.0078,  ..., 0.7020, 0.8275, 0.7216],\n",
      "         ...,\n",
      "         [0.0471, 0.0314, 0.0275,  ..., 0.1294, 0.1373, 0.1373],\n",
      "         [0.0667, 0.0275, 0.0275,  ..., 0.2039, 0.2235, 0.2196],\n",
      "         [0.0627, 0.0275, 0.0745,  ..., 0.3725, 0.2275, 0.2118]],\n",
      "\n",
      "        [[0.0431, 0.0392, 0.0353,  ..., 0.6235, 0.5725, 0.4431],\n",
      "         [0.0353, 0.0235, 0.0235,  ..., 0.6235, 0.7098, 0.5608],\n",
      "         [0.0275, 0.0353, 0.0118,  ..., 0.6392, 0.7412, 0.6353],\n",
      "         ...,\n",
      "         [0.0471, 0.0314, 0.0275,  ..., 0.1098, 0.1059, 0.1176],\n",
      "         [0.0667, 0.0275, 0.0275,  ..., 0.1608, 0.1765, 0.1765],\n",
      "         [0.0627, 0.0314, 0.0784,  ..., 0.3020, 0.1725, 0.1569]]]), 'text': \"This is one of Wtewael's rare small-scale paintings of historical subjects on copper\"}, {'image': tensor([[[0.5216, 0.5216, 0.4863,  ..., 0.3490, 0.3529, 0.3804],\n",
      "         [0.5490, 0.5059, 0.5059,  ..., 0.3451, 0.3412, 0.3490],\n",
      "         [0.5529, 0.5333, 0.5059,  ..., 0.3373, 0.3333, 0.3412],\n",
      "         ...,\n",
      "         [0.1569, 0.1451, 0.1373,  ..., 0.2039, 0.2196, 0.2392],\n",
      "         [0.1843, 0.1529, 0.1529,  ..., 0.2118, 0.2196, 0.2353],\n",
      "         [0.1804, 0.1647, 0.1725,  ..., 0.2275, 0.2353, 0.2510]],\n",
      "\n",
      "        [[0.4314, 0.4235, 0.3804,  ..., 0.2941, 0.2980, 0.3255],\n",
      "         [0.4588, 0.4039, 0.3961,  ..., 0.2902, 0.2863, 0.2941],\n",
      "         [0.4627, 0.4353, 0.4000,  ..., 0.2824, 0.2784, 0.2863],\n",
      "         ...,\n",
      "         [0.1608, 0.1490, 0.1412,  ..., 0.2000, 0.2157, 0.2353],\n",
      "         [0.1882, 0.1569, 0.1569,  ..., 0.2078, 0.2157, 0.2314],\n",
      "         [0.1843, 0.1686, 0.1765,  ..., 0.2196, 0.2275, 0.2431]],\n",
      "\n",
      "        [[0.3608, 0.3451, 0.2980,  ..., 0.2510, 0.2549, 0.2824],\n",
      "         [0.3882, 0.3294, 0.3137,  ..., 0.2471, 0.2431, 0.2510],\n",
      "         [0.3922, 0.3569, 0.3137,  ..., 0.2392, 0.2353, 0.2431],\n",
      "         ...,\n",
      "         [0.1686, 0.1569, 0.1490,  ..., 0.1922, 0.2078, 0.2275],\n",
      "         [0.2000, 0.1686, 0.1686,  ..., 0.2039, 0.2118, 0.2275],\n",
      "         [0.2039, 0.1882, 0.1961,  ..., 0.2235, 0.2314, 0.2471]]]), 'text': 'Catalogue number: Bredius 2.Throughout his long career, Rembrandt continued to produce self-portraits in a vast number of drawings, etchings and paintings, but his intentions and ideas concerning them fundamentally changed over the years. In the present 1629 self-portrait Rembrandt painted himself in a slightly stooped posture with the intention of evoking a sense of movement and spontaneity. Now aged twenty-three, he gazes intently at us with his eyebrows raised and his lips slightly parted, as if in a state of sudden bewilderment or inquisitive interaction with the viewer'}, {'image': tensor([[[0.5373, 0.5176, 0.4863,  ..., 0.4471, 0.4392, 0.4235],\n",
      "         [0.5137, 0.4980, 0.5020,  ..., 0.4588, 0.4549, 0.4353],\n",
      "         [0.5059, 0.5529, 0.6314,  ..., 0.4235, 0.4314, 0.4471],\n",
      "         ...,\n",
      "         [0.3294, 0.3451, 0.3451,  ..., 0.2863, 0.2863, 0.2784],\n",
      "         [0.2275, 0.2627, 0.2784,  ..., 0.2039, 0.2000, 0.1843],\n",
      "         [0.2314, 0.2431, 0.2431,  ..., 0.1922, 0.1804, 0.1882]],\n",
      "\n",
      "        [[0.4863, 0.4980, 0.4863,  ..., 0.4314, 0.4392, 0.4353],\n",
      "         [0.4824, 0.4784, 0.4824,  ..., 0.4196, 0.4431, 0.4431],\n",
      "         [0.4588, 0.5059, 0.5843,  ..., 0.3922, 0.4118, 0.4392],\n",
      "         ...,\n",
      "         [0.2784, 0.2941, 0.2941,  ..., 0.2275, 0.2275, 0.2157],\n",
      "         [0.1922, 0.2235, 0.2392,  ..., 0.1529, 0.1490, 0.1333],\n",
      "         [0.2000, 0.2118, 0.2157,  ..., 0.1529, 0.1412, 0.1412]],\n",
      "\n",
      "        [[0.4510, 0.4627, 0.4510,  ..., 0.3176, 0.3451, 0.3686],\n",
      "         [0.4627, 0.4392, 0.4314,  ..., 0.3608, 0.3804, 0.3804],\n",
      "         [0.4314, 0.4431, 0.5137,  ..., 0.3569, 0.3686, 0.3804],\n",
      "         ...,\n",
      "         [0.2431, 0.2588, 0.2588,  ..., 0.1804, 0.1843, 0.1804],\n",
      "         [0.1529, 0.1843, 0.2000,  ..., 0.1176, 0.1137, 0.1020],\n",
      "         [0.1608, 0.1725, 0.1725,  ..., 0.1137, 0.1059, 0.1176]]]), 'text': 'The background and the sky are characteristic of the artist'}, {'image': tensor([[[0.5020, 0.4706, 0.4824,  ..., 0.3294, 0.3333, 0.3569],\n",
      "         [0.5569, 0.5294, 0.5020,  ..., 0.3686, 0.3922, 0.3922],\n",
      "         [0.6000, 0.5373, 0.5176,  ..., 0.4275, 0.4118, 0.4118],\n",
      "         ...,\n",
      "         [0.1961, 0.1961, 0.1843,  ..., 0.2863, 0.2941, 0.2863],\n",
      "         [0.1961, 0.2118, 0.2039,  ..., 0.2980, 0.3137, 0.3020],\n",
      "         [0.2157, 0.2235, 0.2353,  ..., 0.3294, 0.3451, 0.3608]],\n",
      "\n",
      "        [[0.5255, 0.4941, 0.5059,  ..., 0.2941, 0.2980, 0.3255],\n",
      "         [0.5765, 0.5490, 0.5176,  ..., 0.3412, 0.3686, 0.3725],\n",
      "         [0.6157, 0.5529, 0.5333,  ..., 0.4196, 0.4078, 0.4078],\n",
      "         ...,\n",
      "         [0.1608, 0.1608, 0.1490,  ..., 0.2157, 0.2235, 0.2157],\n",
      "         [0.1608, 0.1765, 0.1686,  ..., 0.2235, 0.2353, 0.2235],\n",
      "         [0.1804, 0.1882, 0.2000,  ..., 0.2431, 0.2549, 0.2627]],\n",
      "\n",
      "        [[0.4275, 0.3961, 0.4078,  ..., 0.2275, 0.2275, 0.2392],\n",
      "         [0.5020, 0.4745, 0.4431,  ..., 0.2745, 0.2941, 0.2824],\n",
      "         [0.5529, 0.4902, 0.4706,  ..., 0.3490, 0.3255, 0.3137],\n",
      "         ...,\n",
      "         [0.1412, 0.1412, 0.1294,  ..., 0.1686, 0.1765, 0.1686],\n",
      "         [0.1412, 0.1569, 0.1490,  ..., 0.1765, 0.1922, 0.1804],\n",
      "         [0.1569, 0.1608, 0.1686,  ..., 0.1961, 0.2078, 0.2196]]]), 'text': 'A repetition painted after the original made for the Hospital de Tavera in Toledo in 1595'}, {'image': tensor([[[0.2314, 0.2941, 0.3176,  ..., 0.3804, 0.4039, 0.4471],\n",
      "         [0.2353, 0.3020, 0.3059,  ..., 0.3333, 0.3686, 0.4353],\n",
      "         [0.2745, 0.2902, 0.2902,  ..., 0.3255, 0.3647, 0.4549],\n",
      "         ...,\n",
      "         [0.2157, 0.2196, 0.2196,  ..., 0.1843, 0.1961, 0.2118],\n",
      "         [0.2353, 0.2275, 0.2196,  ..., 0.2000, 0.2118, 0.2196],\n",
      "         [0.2627, 0.2549, 0.2353,  ..., 0.2314, 0.2392, 0.2431]],\n",
      "\n",
      "        [[0.1843, 0.2392, 0.2510,  ..., 0.3176, 0.3294, 0.3843],\n",
      "         [0.1882, 0.2471, 0.2392,  ..., 0.2706, 0.2902, 0.3686],\n",
      "         [0.2275, 0.2353, 0.2275,  ..., 0.2627, 0.2902, 0.3843],\n",
      "         ...,\n",
      "         [0.1804, 0.1882, 0.1882,  ..., 0.1843, 0.1961, 0.2118],\n",
      "         [0.2118, 0.2039, 0.1961,  ..., 0.2000, 0.2118, 0.2157],\n",
      "         [0.2353, 0.2275, 0.2078,  ..., 0.2314, 0.2392, 0.2431]],\n",
      "\n",
      "        [[0.1725, 0.2196, 0.2275,  ..., 0.3059, 0.3216, 0.3804],\n",
      "         [0.1765, 0.2275, 0.2157,  ..., 0.2627, 0.2863, 0.3686],\n",
      "         [0.2157, 0.2157, 0.2000,  ..., 0.2549, 0.2863, 0.3843],\n",
      "         ...,\n",
      "         [0.1843, 0.1922, 0.1922,  ..., 0.1922, 0.2039, 0.2196],\n",
      "         [0.2235, 0.2157, 0.2078,  ..., 0.2078, 0.2196, 0.2275],\n",
      "         [0.2431, 0.2314, 0.2157,  ..., 0.2392, 0.2471, 0.2510]]]), 'text': \"This still-life was painted by the artist during his later years. In 1728 he was accepted as a painter of animals and fruit at the Paris Academy of Art without having to fulfil the usual requirements.The structure of this painting is simpler than in his earlier still-lifes, and Chardin has reduced the number of objects to a minimum. By singling out and thus monumentalizing the motif of the bird, Chardin gives it considerably more emphasis. According to the categories of feudal game law, the pheasant was seen as reserved for the nobility, but the hunting trophy which has been attached to the pheasant has, from a bourgeois point of view, lost its value of triumphantly demonstrating man's lordship over nature. However, the way in which the pheasant is rendered does not indicate in any way that colour is gradually becoming detached from the object. Rather, the careful, delicate application of the paint - even in the more roughened structures - heightens the element of sensitive empathy. Unlike the game still-lifes of his contemporaries - which have a smooth, cold objectiveness about them - the artist has created an atmosphere of intimacy between the viewer and the object.\"}, {'image': tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.2235, 0.2314, 0.2275,  ..., 0.0471, 0.0667, 0.0745],\n",
      "         [0.1961, 0.2000, 0.1961,  ..., 0.0549, 0.0784, 0.0902],\n",
      "         [0.1922, 0.1843, 0.1765,  ..., 0.0706, 0.0863, 0.0902]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.2039, 0.2118, 0.2078,  ..., 0.0667, 0.0863, 0.0824],\n",
      "         [0.1765, 0.1843, 0.1765,  ..., 0.0745, 0.0980, 0.1020],\n",
      "         [0.1843, 0.1765, 0.1686,  ..., 0.0824, 0.0980, 0.0941]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.1843, 0.1922, 0.1922,  ..., 0.0824, 0.1020, 0.1020],\n",
      "         [0.1765, 0.1843, 0.1765,  ..., 0.0902, 0.1137, 0.1216],\n",
      "         [0.1882, 0.1804, 0.1725,  ..., 0.1020, 0.1176, 0.1137]]]), 'text': \"Baptism of Christ is the only Veronese's painting in Santissimo Redentore in Venice. It stands in the Sacristy of the church\"}, {'image': tensor([[[0.2667, 0.2706, 0.2784,  ..., 0.2353, 0.2353, 0.2667],\n",
      "         [0.2667, 0.2824, 0.2824,  ..., 0.2118, 0.2235, 0.2510],\n",
      "         [0.2863, 0.2784, 0.2706,  ..., 0.2078, 0.2157, 0.2392],\n",
      "         ...,\n",
      "         [0.1569, 0.1569, 0.1725,  ..., 0.1333, 0.1373, 0.1922],\n",
      "         [0.1608, 0.1647, 0.1804,  ..., 0.1490, 0.1647, 0.2235],\n",
      "         [0.1569, 0.1451, 0.1608,  ..., 0.1961, 0.1922, 0.2118]],\n",
      "\n",
      "        [[0.2902, 0.2824, 0.2863,  ..., 0.2667, 0.2784, 0.3059],\n",
      "         [0.2784, 0.2863, 0.2745,  ..., 0.2431, 0.2627, 0.2902],\n",
      "         [0.2824, 0.2745, 0.2588,  ..., 0.2392, 0.2588, 0.2824],\n",
      "         ...,\n",
      "         [0.2000, 0.1843, 0.1725,  ..., 0.2118, 0.2235, 0.2471],\n",
      "         [0.2039, 0.1922, 0.1882,  ..., 0.2196, 0.2353, 0.2667],\n",
      "         [0.2118, 0.2000, 0.2078,  ..., 0.2510, 0.2588, 0.2784]],\n",
      "\n",
      "        [[0.2980, 0.2824, 0.2745,  ..., 0.2588, 0.2667, 0.3098],\n",
      "         [0.2745, 0.2745, 0.2588,  ..., 0.2314, 0.2510, 0.2902],\n",
      "         [0.2706, 0.2549, 0.2314,  ..., 0.2275, 0.2471, 0.2863],\n",
      "         ...,\n",
      "         [0.2235, 0.2000, 0.1882,  ..., 0.1922, 0.2157, 0.2510],\n",
      "         [0.2275, 0.2078, 0.2039,  ..., 0.2157, 0.2275, 0.2588],\n",
      "         [0.2392, 0.2235, 0.2314,  ..., 0.2549, 0.2667, 0.2784]]]), 'text': \"Godfried Schalcken turned to portraiture in 1676, using the half-length and knee-length variety introduced and mad fashionable by Bartholomeus van der Helst in the middle of the century, incorporating suitable accoutrements to indicate the sitter's rank. This style of portrait, so popular in the last decades of the seventeenth century is represented by the present portrait\"}, {'image': tensor([[[0.1961, 0.1843, 0.1804,  ..., 0.2118, 0.2196, 0.2314],\n",
      "         [0.1882, 0.1804, 0.1765,  ..., 0.2039, 0.2196, 0.2431],\n",
      "         [0.1843, 0.1804, 0.1804,  ..., 0.1961, 0.1961, 0.2039],\n",
      "         ...,\n",
      "         [0.1294, 0.1216, 0.1137,  ..., 0.1765, 0.1843, 0.1922],\n",
      "         [0.1176, 0.1333, 0.1137,  ..., 0.1765, 0.1922, 0.2039],\n",
      "         [0.1216, 0.1255, 0.1255,  ..., 0.1922, 0.2078, 0.2510]],\n",
      "\n",
      "        [[0.1490, 0.1373, 0.1333,  ..., 0.1529, 0.1608, 0.1725],\n",
      "         [0.1412, 0.1333, 0.1294,  ..., 0.1451, 0.1608, 0.1843],\n",
      "         [0.1373, 0.1333, 0.1333,  ..., 0.1373, 0.1373, 0.1451],\n",
      "         ...,\n",
      "         [0.1059, 0.0980, 0.0902,  ..., 0.1216, 0.1294, 0.1412],\n",
      "         [0.0941, 0.1098, 0.0902,  ..., 0.1216, 0.1373, 0.1529],\n",
      "         [0.0980, 0.1020, 0.1020,  ..., 0.1373, 0.1529, 0.2000]],\n",
      "\n",
      "        [[0.1647, 0.1529, 0.1490,  ..., 0.1647, 0.1725, 0.1804],\n",
      "         [0.1569, 0.1490, 0.1451,  ..., 0.1569, 0.1725, 0.1922],\n",
      "         [0.1529, 0.1490, 0.1490,  ..., 0.1490, 0.1490, 0.1608],\n",
      "         ...,\n",
      "         [0.1216, 0.1137, 0.1059,  ..., 0.1176, 0.1255, 0.1333],\n",
      "         [0.1098, 0.1255, 0.1059,  ..., 0.1176, 0.1333, 0.1490],\n",
      "         [0.1137, 0.1176, 0.1176,  ..., 0.1333, 0.1490, 0.2000]]]), 'text': 'Alonso de Ercilla y Z�niga (1533-1594) was a Spanish soldier and poet, author of La Araucana (1569-89), the most celebrated Spanish Renaissance epic poem and the first epic poem about America'}]\n"
     ]
    }
   ],
   "source": [
    "print(data_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dee676e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'text'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset = DatasetDict({\"train\": datasets.Dataset.from_list(data_var)})\n",
    "hf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbcf965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(example_batch):\n",
    "#     ((V + 1.0) * 255/2).astype(np.uint8)\n",
    "    images = np.asarray([x for x in example_batch[\"image\"]])\n",
    "    captions = [x for x in example_batch[\"text\"]]\n",
    "    inputs = processor(images=images, text=captions, padding=\"max_length\")\n",
    "    inputs.update({\"labels\": inputs[\"input_ids\"]})\n",
    "    return inputs\n",
    "\n",
    "hf_dataset.set_transform(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "963cfa5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'text'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e49ae25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom collator to handle the format expected by the BLIP model\n",
    "def collate_fn(batch):\n",
    "#     print(f'batch {batch}')\n",
    "    images = [item['image'] for item in batch]\n",
    "    texts = [item['text'] for item in batch]\n",
    "    # Convert the images into tensor and normalize\n",
    "    images = torch.stack(images)\n",
    "    images = torch.add(images,1.0)\n",
    "    text = torch.stack(texts)\n",
    "    return processor(images=images, text=texts, padding=True, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ee13099",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'do_rescale'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Define Training Arguments\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./blip-finetuned\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;43;03m#     evaluation_strategy=\"steps\",\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./logs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set to True if uploading to Hugging Face Hub\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_unused_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_cuda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_rescale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'do_rescale'"
     ]
    }
   ],
   "source": [
    "# Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./blip-finetuned\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    save_steps=500,\n",
    "    evaluation_strategy=None,\n",
    "#     evaluation_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    push_to_hub=False,  # Set to True if uploading to Hugging Face Hub\n",
    "    logging_steps=10,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    remove_unused_columns=False,\n",
    "    max_steps=10,\n",
    "    no_cuda=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fbab7524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pretrained BLIP Model & Processor\n",
    "model_name = \"Salesforce/blip-image-captioning-base\"\n",
    "processor = BlipProcessor.from_pretrained(model_name)\n",
    "model = BlipForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4f95ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=hf_dataset[\"train\"].with_format(\"torch\"),\n",
    "#     tokenizer=processor,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "271cda9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-25-08206ab134dc>:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset= hf_dataset[\"train\"],\n",
    "    tokenizer=processor,\n",
    "#     data_collator=collate_fn,  # Use custom collate function to prepare batches\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "499864f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4547a655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 03:15, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>9.469500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=9.469465637207032, metrics={'train_runtime': 232.8653, 'train_samples_per_second': 0.344, 'train_steps_per_second': 0.043, 'total_flos': 2.96711826112512e+16, 'train_loss': 9.469465637207032, 'epoch': 5.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start Fine-Tuning\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
